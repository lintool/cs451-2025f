{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78262290",
   "metadata": {},
   "source": [
    "# CS 451/651: Data-Intensive Distributed Computing (Fall 2025)\n",
    "# Assignment 2: ETL\n",
    "\n",
    "Read the setup of this assignment in the <a href=\"https://lintool.github.io/cs451-2025f/assignments/assignment2.html\">assignment 2 landing page</a> before you get started here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1e1996-383c-4a4a-b3da-30edcd6e1762",
   "metadata": {},
   "source": [
    "## 1. Querying the Operational Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183ab400",
   "metadata": {},
   "source": [
    "Let's run a query to verify that the operational database has been properly restored and that we can issue a query to PostgreSQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a33f61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!psql \"cs451\" -v ON_ERROR_STOP=1 -c \"SELECT COUNT(DISTINCT user_id) AS number_users FROM retail.user;\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3851c65a-e658-4044-8878-4c3d3722463e",
   "metadata": {},
   "source": [
    "**The correct answer should be 3022290.**\n",
    "\n",
    "If running the cell above gives you the same answer, the everything should be in order.\n",
    "\n",
    "If you're getting an error, fix it before moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d489b61-7304-4513-8c79-89c07745a3ec",
   "metadata": {},
   "source": [
    "As a warmup exercise, write SQL queries against the operational database to answer the following questions and report the answers.\n",
    "Place both your SQL queries and answers in the following cell, replacing the placeholder texts that exist there.\n",
    "Each question needs to be answered by a _single_ SQL query (that is, it is not acceptable to run multiple SQL queries and then compute the answer yourself).\n",
    "\n",
    "1. For `session_id` `789d3699-028e-4367-b515-b82e2cb5225f`, what was the purchase price?\n",
    "2. How many products are sold by the brand \"sokolov\"?\n",
    "3. What is the average purchase price of items purchased from the brand \"febest\"?\n",
    "4. What is average number of events per user? (Report answer to two digits after the decimal point, i.e., XX.XX)\n",
    "\n",
    "<font color=\"red\">**Below, you'll have to write some code!**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b515132-07e2-44e3-abea-287b0a89af4a",
   "metadata": {},
   "source": [
    "// qcell_1b76x2 (keep this id for tracking purposes)\n",
    "\n",
    "**Q1 SQL:**\n",
    "\n",
    "SELECT....\n",
    "\n",
    "**Q1 answer:**\n",
    "\n",
    "XXXX\n",
    "\n",
    "**Q2 SQL:**\n",
    "\n",
    "SELECT....\n",
    "\n",
    "**Q2 answer:**\n",
    "\n",
    "XXXX\n",
    "\n",
    "**Q3 SQL:**\n",
    "\n",
    "SELECT....\n",
    "\n",
    "**Q3 answer:**\n",
    "\n",
    "XXXX\n",
    "\n",
    "**Q4 SQL:**\n",
    "\n",
    "SELECT....\n",
    "\n",
    "**Q4 answer:**\n",
    "\n",
    "XXXX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fecda4-fc6e-4767-af8f-c08d72eeaa02",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec996e",
   "metadata": {},
   "source": [
    "The following cell contains setup to measure wall clock time and memory usage. (Don't worry about the details, just run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cbf5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U numpy pandas pyarrow matplotlib scipy\n",
    "import sys, subprocess\n",
    "try:\n",
    "    import psutil  # noqa: F401\n",
    "except Exception:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"psutil\"])\n",
    "print(\"psutil is installed.\")\n",
    "\n",
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "import time, os, platform\n",
    "\n",
    "# Try to import optional modules\n",
    "try:\n",
    "    import psutil\n",
    "except Exception:\n",
    "    psutil = None\n",
    "\n",
    "try:\n",
    "    import resource  # not available on Windows\n",
    "except Exception:\n",
    "    resource = None\n",
    "\n",
    "\n",
    "def _rss_bytes():\n",
    "    \"\"\"Resident Set Size in bytes (cross-platform via psutil if available).\"\"\"\n",
    "    if psutil is not None:\n",
    "        return psutil.Process(os.getpid()).memory_info().rss\n",
    "    # Fallback: unknown RSS → 0 \n",
    "    return 0\n",
    "\n",
    "\n",
    "def _peak_bytes():\n",
    "    \"\"\"\n",
    "    Best-effort peak memory in bytes.\n",
    "    - Windows: psutil peak working set (peak_wset)\n",
    "    - Linux:   resource.ru_maxrss (KB → bytes)\n",
    "    - macOS:   resource.ru_maxrss (bytes)\n",
    "    Fallback to current RSS if unavailable.\n",
    "    \"\"\"\n",
    "    sysname = platform.system()\n",
    "\n",
    "    # Windows path: use psutil peak_wset if present\n",
    "    if sysname == \"Windows\" and psutil is not None:\n",
    "        mi = psutil.Process(os.getpid()).memory_info()\n",
    "        peak = getattr(mi, \"peak_wset\", None)  # should be available on Windows\n",
    "        if peak is not None:\n",
    "            return int(peak)\n",
    "        return int(mi.rss)\n",
    "\n",
    "    # POSIX path: resource may be available\n",
    "    if resource is not None:\n",
    "        try:\n",
    "            ru = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "            # On Linux ru_maxrss is in kilobytes; on macOS/BSD it is bytes\n",
    "            if sysname == \"Linux\":\n",
    "                return int(ru) * 1024\n",
    "            else:\n",
    "                return int(ru)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Last resort\n",
    "    return _rss_bytes()\n",
    "\n",
    "\n",
    "@register_cell_magic\n",
    "def timemem(line, cell):\n",
    "    \"\"\"\n",
    "    Measure wall time and memory around the execution of this cell.\n",
    "\n",
    "        %%timemem\n",
    "        <your code>\n",
    "\n",
    "    Notes:\n",
    "    - RSS = resident memory after the cell.\n",
    "    - Peak is OS-dependent (see _peak_bytes docstring).\n",
    "    \"\"\"\n",
    "    ip = get_ipython()\n",
    "\n",
    "    rss_before  = _rss_bytes()\n",
    "    peak_before = _peak_bytes()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # Execute the cell body\n",
    "    result = ip.run_cell(cell)\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    rss_after  = _rss_bytes()\n",
    "    peak_after = _peak_bytes()\n",
    "\n",
    "    wall = t1 - t0\n",
    "    rss_delta_mb  = (rss_after  - rss_before)  / (1024 * 1024)\n",
    "    peak_delta_mb = (peak_after - peak_before) / (1024 * 1024)\n",
    "\n",
    "    print(\"======================================\")\n",
    "    print(f\"Wall time: {wall:.3f} s\")\n",
    "    print(f\"RSS Δ: {rss_delta_mb:+.2f} MB\")\n",
    "    print(f\"Peak memory Δ: {peak_delta_mb:+.2f} MB (OS-dependent)\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620ca1f2-5c25-49df-9570-f5e56782b55c",
   "metadata": {},
   "source": [
    "## 3. The \"Extract\" in ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acb0763",
   "metadata": {},
   "source": [
    "The operational database comprises the tables described [here](https://lintool.github.io/cs451-2025f/assignments/assignment2.html).\n",
    "\n",
    "For the \"Extract\" in ETL, we're going to extract the following CSV files, each corresponding to a table in the operational database:\n",
    "\n",
    "- **user.csv**: `user_id, gender, birthdate`\n",
    "- **session.csv**: `session_id, user_id`\n",
    "- **product.csv**: `product_id, brand, category, product_name`\n",
    "- **product_name.csv**: `category, product_name, description`\n",
    "- **events.csv**: `event_time, event_type, session_id, product_id, price`\n",
    "- **category.csv**: `category, description`\n",
    "- **brand.csv**: `brand, description`\n",
    "\n",
    "From these files, we'll build a data warehouse organized in a standard star schema that has the following tables:\n",
    "\n",
    "- Dimension tables: `dim_user`, `dim_age`, `dim_brand`, `dim_category`, `dim_product`, `dim_date`, `dim_session`\n",
    "- The main fact table `fact_events` with foreign keys: `date_key, user_key, age_key, product_key, brand_key, category_key, session_key`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d94b96-231a-4db7-9c60-5ae521a38aae",
   "metadata": {},
   "source": [
    "Let's specify a \"base directory\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd673d3-8c30-489a-822c-27419c1e1a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to path on your local machine.\n",
    "BASE_DIR = \"/Users/jimmylin/cs451/a2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f25b07b-0a28-499a-9817-0630640aea20",
   "metadata": {},
   "source": [
    "These are the commands that perform the \"extraction\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626f430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!psql cs451 -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"user\"         TO '\\''{BASE_DIR}/user.csv'\\''         WITH (FORMAT csv, HEADER true)'\n",
    "!psql cs451 -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"session\"      TO '\\''{BASE_DIR}/session.csv'\\''      WITH (FORMAT csv, HEADER true)'\n",
    "!psql cs451 -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"category\"     TO '\\''{BASE_DIR}/category.csv'\\''     WITH (FORMAT csv, HEADER true)'\n",
    "!psql cs451 -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"brand\"        TO '\\''{BASE_DIR}/brand.csv'\\''        WITH (FORMAT csv, HEADER true)'\n",
    "!psql cs451 -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"product_name\" TO '\\''{BASE_DIR}/product_name.csv'\\'' WITH (FORMAT csv, HEADER true)'\n",
    "!psql cs451 -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"product\"      TO '\\''{BASE_DIR}/product.csv'\\''      WITH (FORMAT csv, HEADER true)'\n",
    "!psql cs451 -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"events\"       TO '\\''{BASE_DIR}/events.csv'\\''       WITH (FORMAT csv, HEADER true)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f0dde-6c17-48ad-aa4e-6e4019b106bc",
   "metadata": {},
   "source": [
    "(Note that the quote style above will _not_ work for Windows machines. Please adjust accordingly.)\n",
    "\n",
    "After the extraction, you should have 7 CSV files, each corresponding to a table in the operational database.\n",
    "\n",
    "The CSV files should be stored in `BASE_DIR`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0930cdf5-b989-4608-8a03-22c0eab6b031",
   "metadata": {},
   "source": [
    "The following code snippet should \"just work\" to initialize Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe303ff-e323-4389-a64f-d9e730fa1ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark, os\n",
    "\n",
    "# Change to path on your local machine.\n",
    "os.environ[\"SPARK_HOME\"] = \"/Users/jimmylin/Dropbox/workspace/teaching/spark-4.0.0-bin-hadoop3\"\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "py = sys.executable  # the Python of this notebook (e.g., .../envs/yourenv/bin/python)\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = py\n",
    "os.environ[\"PYSPARK_PYTHON\"] = py\n",
    "\n",
    "spark = SparkSession.getActiveSession() or (\n",
    "    SparkSession.builder\n",
    "    .appName(\"A2\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")           # or 12g+\n",
    "    .config(\"spark.sql.shuffle.partitions\",\"400\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.pyspark.driver.python\", py)\n",
    "    .config(\"spark.pyspark.python\", py)\n",
    "    .config(\"spark.executorEnv.PYSPARK_PYTHON\", py)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec616b-b333-4eb6-8d7e-ab002af8f668",
   "metadata": {},
   "source": [
    "At this point, Spark should be initialized.\n",
    "\n",
    "Let's then load in CSV files into DataFrames.\n",
    "\n",
    "<font color=\"red\">**Below, you'll have to write some code!**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49534dee-f56a-4f6b-8930-64092a0fcba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_30z8le (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "# By the time we get to here, we've loaded each of the CSV files into a corresponding dataframe.\n",
    "# Let's count the number of records in each:\n",
    "\n",
    "print(f\"user: {df_user.count()}\")\n",
    "print(f\"session: {df_session.count()}\")\n",
    "print(f\"product: {df_product.count()}\")\n",
    "print(f\"product_name: {df_product_name.count()}\")\n",
    "print(f\"events: {df_events.count()}\")\n",
    "print(f\"category: {df_category.count()}\")\n",
    "print(f\"brand: {df_brand.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4552f156-6f37-4938-8e93-1a02a38ba760",
   "metadata": {},
   "source": [
    "How do you know if you've done everything correctly?\n",
    "\n",
    "Well, issue the SQL query `select count(*) from retail.user;` to count the number of rows in the `user` table in the operational database.\n",
    "It should match the output of `df_user.count()`; same for the other tables.\n",
    "If the counts match, then you know everything is in order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e10f63",
   "metadata": {},
   "source": [
    "## 4. Build the Dimensions Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3303f4-e9be-455b-a458-91de0ebdded7",
   "metadata": {},
   "source": [
    "### 4.1 The `user` Dimension Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f4ee83",
   "metadata": {},
   "source": [
    "Build the `dim_user` dimension table.\n",
    "This table should include `user_key`, `user_id`, `gender`, `birthdate`, and `generation`. \n",
    "\n",
    "Set `generation` to one of the following values based on the birth year: \n",
    "- \"Traditionalists\": born 1925 to 1945\n",
    "- \"Boomers\": born 1946 to 1964\n",
    "- \"GenX\": born 1965 to 1980\n",
    "- \"Millennials\": born 1981 to 2000\n",
    "- \"GenZ\": born 2001 to 2020\n",
    "\n",
    "<font color=\"red\">**Below, you'll have to write some code!**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883cc05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_41ax14 (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "# By the time we get to here, \"dim_user\" should hold the user dimensions table according to the specification above.\n",
    "\n",
    "dim_user.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e6e753-8515-4924-b4d7-f4a8eeb3c4f8",
   "metadata": {},
   "source": [
    "**The correct answer should be 3022290.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eadced-3481-480a-9320-a07bbfc6d6b7",
   "metadata": {},
   "source": [
    "### 4.2 The `age` Dimension Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6758f1e8",
   "metadata": {},
   "source": [
    "Even though `birthdate` exists in `dim_user`, a separate `dim_age` is helpful because it:\n",
    "- Simplifies analysis with ready-made bands.\n",
    "- Ensures consistency across all queries.\n",
    "- Improves performance via small surrogate keys.\n",
    "- Preserves history by fixing age at event time.\n",
    "- Adds flexibility to adjust bands without changing facts.\n",
    "\n",
    "We're going to build a `dim_age` table that has 4 columns:\n",
    "- `age_key`: (INT, surrogate PK)\n",
    "- `age_band`: (STRING) following the age band rules below\n",
    "- `min_age`: (INT)\n",
    "- `max_age`: (INT)\n",
    "\n",
    "Bands:\n",
    "- \"<18\": min_age = NULL, max_age = 17\n",
    "- \"18-24\": 18, 24\n",
    "- \"25-34\": 25, 34\n",
    "- \"35-44\": 35, 44\n",
    "- \"45-54\": 45, 54\n",
    "- \"55-64\": 55, 64\n",
    "- \"65-74\": 65, 74\n",
    "- \"75-84\": 75, 84\n",
    "- \"85-94\": 85, 94\n",
    "- \"unknown\": NULL, NULL\n",
    "\n",
    "The construction of this table is a bit tricky, so we're going to show you how to do it, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed16029",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "\n",
    "# Static age bands\n",
    "age_band_rows = [\n",
    "    (\"<18\",   None, 17),\n",
    "    (\"18-24\", 18, 24),\n",
    "    (\"25-34\", 25, 34),\n",
    "    (\"35-44\", 35, 44),\n",
    "    (\"45-54\", 45, 54),\n",
    "    (\"55-64\", 55, 64),\n",
    "    (\"65-74\", 65, 74),\n",
    "    (\"75-84\", 75, 84),\n",
    "    (\"85-94\", 85, 94),\n",
    "    (\"unknown\", None, None),\n",
    "]\n",
    "dim_age = spark.createDataFrame(age_band_rows, [\"age_band\", \"min_age\", \"max_age\"])\n",
    "\n",
    "w_age = Window.orderBy(F.col(\"age_band\"))\n",
    "dim_age = dim_age.withColumn(\"age_key\", F.dense_rank().over(w_age))\n",
    "\n",
    "dim_age.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55857d00-560c-4beb-8d29-8626369a3110",
   "metadata": {},
   "source": [
    "**The correct answer should be 10.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6012e1ca-e6ca-46ae-a873-7f5771cd4310",
   "metadata": {},
   "source": [
    "### 4.3 The `brand`, `product`, and `category` Dimension Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe029f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Build the following dimension tables:\n",
    "\n",
    "**`dim_brand`:**\n",
    "- `brand_key` (INT, surrogate PK)\n",
    "- `brand_code` (STRING) \n",
    "- `brand_desc` (STRING)\n",
    "\n",
    "**`dim_category`:**\n",
    "- `category_key` (INT, surrogate PK)\n",
    "- `category_code` (STRING) \n",
    "- `category_desc` (STRING)\n",
    "\n",
    "**`dim_product`:**\n",
    "- `product_key`  (INT, surrogate PK)\n",
    "- `product_id`   (STRING)\n",
    "- `product_desc` (STRING)\n",
    "- `brand_key`   (INT, FK → `dim_brand`)  \n",
    "- `category_key`(INT, FK → `dim_category`)\n",
    "\n",
    "The objective of `dim_product` is to keep all products in `product`, and add details from `product_names`, then join the results with `brand` and `category` dimension tables.\n",
    "\n",
    "<font color=\"red\">**Below, you'll have to write some code!**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acddfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_43k3n9 (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "# By the time we get to here, \"dim_brand\", \"dim_category\", and \"dim_product\" should hold \n",
    "# the dimension tables according to the specifications above.\n",
    "\n",
    "print(f\"Number of rows in dim_brand: {dim_brand.count()}\")\n",
    "print(f\"Number of rows in dim_category: {dim_category.count()}\")\n",
    "print(f\"Number of rows in dim_product: {dim_product.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b819a9-6b4c-4a1f-82a3-e379d1d6a2ad",
   "metadata": {},
   "source": [
    "**Correct answers:**\n",
    "\n",
    "+ Number of rows in `dim_brand`: 3444\n",
    "+ Number of rows in `dim_category`: 13\n",
    "+ Number of rows in `dim_product`: 166794"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60db6b6a-b5af-46ac-ab1c-161cf50ea75a",
   "metadata": {},
   "source": [
    "### 4.4  The `date` Dimension Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504f27d5",
   "metadata": {},
   "source": [
    "This table is expected to have one row per calendar date. \n",
    "\n",
    "**`dim_date`:**\n",
    "- `date_key`     (INT, surrogate PK; format YYYYMMDD)\n",
    "- `date`         (DATE, the actual calendar date)\n",
    "- `day`          (INT, 1–31)\n",
    "- `day_of_week`  (INT, 1=Mon … 7=Sun)\n",
    "- `day_name`     (STRING, e.g., Monday)\n",
    "- `is_weekend`   (BOOLEAN)\n",
    "- `week_of_year` (INT, 1–53, ISO week)\n",
    "- `month`        (INT, 1–12)\n",
    "- `month_name`   (STRING, e.g., January)\n",
    "- `quarter`      (INT, 1–4)\n",
    "- `year`         (INT)\n",
    "\n",
    "\n",
    "There are 2025 years, each with 365 days. Do we need to have a table that big? \n",
    "We can, but we do not have to! \n",
    "\n",
    "Instead, follow these instructions to create only as many rows as we need:\n",
    "\n",
    "1. Determine the date range (from the min and max `event_date` in `df_events`).\n",
    "2. Generate all dates in that range with `F.sequence()`.\n",
    "3. Derive attributes (`day`, `day_of_week`, ...).\n",
    "4. Create `date_key` = `year * 10000 + month * 100 + day` (i.e., YYYYMMDD).\n",
    "5. Assign `date_key` as the surrogate PK.\n",
    "\n",
    "Build the `dim_date` table conforming to the specifications above.\n",
    "\n",
    "<font color=\"red\">**Below, you'll have to write some code!**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d0554",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_44qm5c (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "# By the time we get to here, \"dim_date\" should hold the dates dimension table according to the specification above.\n",
    "\n",
    "print(dim_date.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6191b0ce-16fe-4359-bdc2-b23030c5e4f0",
   "metadata": {},
   "source": [
    "**The correct answer should be 32.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e838229",
   "metadata": {},
   "source": [
    "If you reach here, congratulations!\n",
    "You have created all the dimension tables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "\n",
    "print(f\"dim_user: {dim_user.count()}\")\n",
    "print(f\"dim_age: {dim_age.count()}\")\n",
    "print(f\"dim_brand: {dim_brand.count()}\")\n",
    "print(f\"dim_category: {dim_category.count()}\")\n",
    "print(f\"dim_product: {dim_product.count()}\")\n",
    "print(f\"dim_date: {dim_date.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c991409d-c410-46b3-bf2c-b3acb4b5fb28",
   "metadata": {},
   "source": [
    "**Correct answers:**\n",
    "\n",
    "- `dim_user`: 3022290\n",
    "- `dim_age`: 10\n",
    "- `dim_brand`: 3444\n",
    "- `dim_category`: 13\n",
    "- `dim_product`: 166794\n",
    "- `dim_date`: 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1fa63f-101b-444d-9013-20c2947657b4",
   "metadata": {},
   "source": [
    "## 5. Build the Fact Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3523d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Now it's time to build the fact table!\n",
    "\n",
    "Our goal in this step is to create a clean `fact_events` table that joins the events from the operational database to the dimension tables you've just built above.\n",
    "Along the way, we're going to enforce data quality and do a bit of data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb231a1-f368-40b1-aa3c-e448c151bbec",
   "metadata": {},
   "source": [
    "### 5.1 Clean Events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75d9cc-9eb4-47e5-85dd-f073172fc81c",
   "metadata": {},
   "source": [
    "Create `events_clean` by removing any record that \"does not make sense\".\n",
    "Specifically:\n",
    "\n",
    "- Start from the `df_events` DataFrame.\n",
    "- Keep only rows with non-null timestamps, `session_id`, and `product_id`.\n",
    "- Cast price to double; keep `NULL` prices (views/carts can be price-less) and non-negative values only.\n",
    "- Drop dates in the future.\n",
    "- Restrict to valid event types: `view`, `cart`, `purchase`, `remove`.\n",
    "\n",
    "<font color=\"red\">**Below, you'll have to write some code!**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559ae17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_51ep7v (keep this id for tracking purposes)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "from operator import and_ as AND\n",
    "\n",
    "valid_types = [\"view\", \"cart\", \"purchase\", \"remove\"]\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "# By the time we get to here, \"events_clean\" should conform to the specification above.\n",
    "\n",
    "events_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94fd335",
   "metadata": {},
   "source": [
    "### 5.2 Cap Silly Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d6f0df-6101-4c96-a30a-03b498844a20",
   "metadata": {},
   "source": [
    "Next, let us check some statistics about prices and then decide what we want to do.\n",
    "\n",
    "What is the minimum, maximum, and average price in this database?\n",
    "\n",
    "<font color=\"red\">**Below, you'll have to write some code!**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a169cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_52hg6x (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "# By the time we get to here, \"minimum\", \"maximum\", and \"average\" should conform to the specification above.\n",
    "\n",
    "print(f\"minimum: {minimum}\")\n",
    "print(f\"maximum: {maximum}\")\n",
    "print(f\"average: {average}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b48511",
   "metadata": {},
   "source": [
    "Wait, something's not right! \n",
    "The average price is 864.27 but the maximum seems suss...\n",
    "It is possible these high prices are just errors.\n",
    "\n",
    "For simplicity, let us assume a threshold value equal to 100x the average, and remove anything more than that.\n",
    "Filter `events_clean` as described.\n",
    "\n",
    "<font color=\"red\">**Below, you'll have to write some code!**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9783cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_52bf5d (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "# By the time we get to here, \"events_clean\" should conform to the specification above.\n",
    "\n",
    "events_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac33e64b",
   "metadata": {},
   "source": [
    "Good, we still have about 42.4M records, but we've done some basic data cleaning.\n",
    "Let us continue..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3065688-4d49-4567-b7f5-eb866bf52441",
   "metadata": {},
   "source": [
    "### 5.3 Build Tiny Lookup Tables (LKPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b5cb09-baf1-45c4-9b6b-0083c1d86b4b",
   "metadata": {},
   "source": [
    "Create lookup tables that help us connect `events_clean` with the dimension tables we created:\n",
    "\n",
    "- `user_lkp`: (`user_id` → `user_key`) from `dim_user`.\n",
    "- `prod_lkp`:(`product_id` → `product_key`, `brand_key`, `category_key`) from `dim_product`.\n",
    "- `date_lkp`: (`date` → `date_key`) from `dim_date`.\n",
    "- session-to-user bridge: use the raw `df_session` (`session_id`, `user_id`) CSV (not a dimension) to pull `user_id`.\n",
    "\n",
    "**Hint:** These LKPs are just calling `select` from the right sources with the right parameters.\n",
    "\n",
    "<font color=\"red\">**Below, you'll have to write some code!**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056954dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_53l2kp (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "# By the time we get to here, the following variables should conform to the specification above.\n",
    "\n",
    "print(session_bridge.count(), user_lkp.count(), prod_lkp.count(), date_lkp.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b6b4e-0449-4db7-8971-98ceabe29eb9",
   "metadata": {},
   "source": [
    "### 5.4 Join Everything Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad90901",
   "metadata": {},
   "source": [
    "Finally, join everything together to create `fact_events`.\n",
    "Follow the following steps:\n",
    "\n",
    "- Start from `clean events` with these columns: (`event_time`, `event_type`, `session_id`, `product_id`, `price`, `date`).\n",
    "- Join sessions first (to get `user_id`).\n",
    "- Then join product, date, and user.\n",
    "- Join with `dim_user` to find out the birthdate and compute user age at the day of the event in `age_on_event`.\n",
    "- Join with `dim_age` to find the age band based on `age_on_event`.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- You built the LKPs for a reason... use them.\n",
    "- Left, right, or natural joins?\n",
    "\n",
    "The final part above is a bit tricky, so we'll just give you the answer. But you'll need to figure out how it integrates with everything above.\n",
    "\n",
    "```\n",
    "        .withColumn(\"age_on_event\", F.floor(F.months_between(F.col(\"date\"), F.to_date(\"birthdate\"))/12))\n",
    "        .join(\n",
    "           dim_age.select(\"age_key\", \"age_band\", \"min_age\", \"max_age\"),\n",
    "           (\n",
    "               ((F.col(\"age_on_event\") > F.col(\"min_age\"))) &\n",
    "               ((F.col(\"age_on_event\") <= F.col(\"max_age\")))\n",
    "           ),\n",
    "           \"left\"\n",
    "       )\n",
    "```\n",
    "\n",
    "The final result (`fact_events`) should include the following columns:\n",
    "\n",
    "- `date_key`\n",
    "- `user_key`\n",
    "- `age_key`\n",
    "- `product_key`\n",
    "- `brand_key`\n",
    "- `category_key`\n",
    "- `session_id`\n",
    "- `event_time`\n",
    "- `event_type`\n",
    "- `price`\n",
    "\n",
    "<font color=\"red\">**Below, you'll have to write some code!**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4660de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_54aaaa (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "# By the time we get to here, \"fact_events\" should conform to the specification above.\n",
    "\n",
    "print(fact_events.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abdfc6c",
   "metadata": {},
   "source": [
    "Congrats, you've done it!\n",
    "You've created the fact table successfuly! 🚀\n",
    "\n",
    "Here is the summary of the schema:\n",
    "\n",
    "- `date_key` (FK → `dim_date`)\n",
    "- `user_key` (FK → `dim_user`)\n",
    "- `age_key`  (FK → `dim_age`)\n",
    "- `product_key` (FK → `dim_product`)\n",
    "- `brand_key` (FK → `dim_brand`)\n",
    "- `category_key` (FK → `dim_category`)\n",
    "- `session_id` (STRING, business key, kept directly in this table)\n",
    "- `event_time` (TIMESTAMP)\n",
    "- `event_tpe` (STRING)\n",
    "- `price` (DOUBLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa5c0a-b7ab-4340-9b05-1dc615444254",
   "metadata": {},
   "source": [
    "## 6. Export the Fact Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ebb4ea",
   "metadata": {},
   "source": [
    "You now have a shiny `fact_events` table!\n",
    "But how should you store it?\n",
    "(Remember our discussion in class about row vs. column representations?)\n",
    "\n",
    "Let's store `fact_events` in a few different ways and compare data sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb22a99-f504-4487-97c3-324554333091",
   "metadata": {},
   "source": [
    "First, let's try writing out as CSV files, both compressed and uncompressed, per below.\n",
    "\n",
    "Note that in Spark, we specify the output _directory_, which is then populated with many \"part\" files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb42721a-f950-411d-93ce-2069629f019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_events.write.mode(\"overwrite\").option(\"header\", True).csv(BASE_DIR + \"/fact_events.csv\")\n",
    "fact_events.write.mode(\"overwrite\").option(\"header\", True).option(\"compression\", \"snappy\").csv(BASE_DIR + \"/fact_events.csv.snappy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb2541-f412-437f-8b1f-d973ab722ab5",
   "metadata": {},
   "source": [
    "Let's then try Parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9721b6-d435-4438-bf06-131a8d0a4384",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_events.write.mode(\"overwrite\").parquet(BASE_DIR + \"/fact_events.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d8161-114a-4896-b5b9-aa7a27c02e76",
   "metadata": {},
   "source": [
    "Let's compare the output sizes using the following bit of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453bd520-4a21-4161-b6c2-6758bb6546e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for f in [BASE_DIR + \"/fact_events.csv\", BASE_DIR + \"/fact_events.csv.snappy\", BASE_DIR + \"/fact_events.parquet\"]:\n",
    "    try:\n",
    "        size = sum(os.path.getsize(os.path.join(dp, fn))\n",
    "                   for dp, dn, filenames in os.walk(f)\n",
    "                   for fn in filenames)\n",
    "        print(f\"{f}: {size/(1024*1024*1024):.1f} GB\")\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d796fa9-cad5-4ca6-b6cd-cbae63cdc96e",
   "metadata": {},
   "source": [
    "<font color=\"red\">**Put your answers below! Replace X.X with the actual answer.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56df9e6-a6bd-4ebd-a692-63c4bd590475",
   "metadata": {},
   "source": [
    "// qcell_6a9876 (keep this id for tracking purposes)\n",
    "\n",
    "- **Size of CSV output, no compression:** <font color=\"red\">X.X</font> GB\n",
    "- **Size of CSV output, Snappy compression:** <font color=\"red\">X.X</font> GB\n",
    "- **Size of Parquet output:** <font color=\"red\">X.X</font> GB "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b7369-6ce3-455a-917b-bc002e112da4",
   "metadata": {},
   "source": [
    "**Answer the following question:**\n",
    "\n",
    "Q6.1 Why is columnar storage (Parquet) usually much smaller?\n",
    "\n",
    "Q6.2 Which format is better for analytical queries and why?\n",
    "\n",
    "<font color=\"red\">**Put your answers below!**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad1b29b-375e-4143-b844-cd40c3183e0f",
   "metadata": {},
   "source": [
    "// qcell_6b1234 (keep this id for tracking purposes)\n",
    "\n",
    "**Q6.1 Answer:**\n",
    "\n",
    "<font color=\"red\">blah blah (replace with your actual answer)</font>\n",
    "\n",
    "**Q6.2 Answer:**\n",
    "\n",
    "<font color=\"red\">blah blah (replace with your actual answer)</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5ab3d-73dd-4bd9-9722-4f07e6b913c0",
   "metadata": {},
   "source": [
    "## 7. Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568bb473",
   "metadata": {},
   "source": [
    "Details about the submission of this assignment are outlined in the <a href=\"https://lintool.github.io/cs451-2025f/assignments/assignment2.html\">assignment 2 landing page</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89841125",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
